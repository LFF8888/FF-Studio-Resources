{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0ZnyCLh-49B"
      },
      "source": [
        "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "<!--- @wandbcode{pytorch-video} -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geV6vA8A-49C"
      },
      "source": [
        "# 🔥 = W&B ➕ PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21pXq2Ka-49C"
      },
      "source": [
        "\n",
        "使用 [Weights & Biases](https://wandb.com) 进行机器学习实验跟踪、数据集版本控制和项目协作。\n",
        "\n",
        "<div><img /></div>\n",
        "\n",
        "<img src=\"https://wandb.me/mini-diagram\" width=\"650\" alt=\"Weights & Biases\" />\n",
        "\n",
        "<div><img /></div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srxCoRpd-49C"
      },
      "source": [
        "\n",
        "## 本笔记本涵盖的内容：\n",
        "\n",
        "我们将向你展示如何将 Weights & Biases 与 PyTorch 代码集成，以便为你的管道添加实验跟踪功能。\n",
        "\n",
        "## 最终生成的交互式 W&B 仪表盘将如下所示：  \n",
        "![](https://i.imgur.com/z8TK2Et.png)\n",
        "\n",
        "## 伪代码中，我们将执行以下操作：  \n",
        "```python\n",
        "# 导入库\n",
        "import wandb\n",
        "\n",
        "# 启动一个新实验\n",
        "wandb.init(project=\"new-sota-model\")\n",
        "\n",
        "# 使用 config 捕获超参数字典\n",
        "wandb.config = {\"learning_rate\": 0.001, \"epochs\": 100, \"batch_size\": 128}\n",
        "\n",
        "# 设置模型和数据\n",
        "model, dataloader = get_model(), get_data()\n",
        "\n",
        "# 可选：跟踪梯度\n",
        "wandb.watch(model)\n",
        "\n",
        "for batch in dataloader:\n",
        "  metrics = model.training_step()\n",
        "  # 在训练循环中记录指标以可视化模型性能\n",
        "  wandb.log(metrics)\n",
        "\n",
        "# 可选：在最后保存模型\n",
        "model.to_onnx()\n",
        "wandb.save(\"model.onnx\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UswhxBa2-49D"
      },
      "source": [
        "\n",
        "\n",
        "## 跟随 [视频教程](http://wandb.me/pytorch-video) 一起学习！  \n",
        "**注意**：以 _Step_ 开头的部分是你在现有管道中集成 W&B 所需的全部内容。其余部分只是加载数据并定义模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrzvygLP-49E"
      },
      "source": [
        "# 🚀 安装、导入和登录"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChlWcqSI-49E"
      },
      "source": [
        "### 0️⃣ 步骤 0：安装 W&B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NuUqbta-49E"
      },
      "source": [
        "首先，我们需要获取库。  \n",
        "`wandb` 可以通过 `pip` 轻松安装。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFSP7YoG-49F"
      },
      "outputs": [],
      "source": [
        "!pip install wandb onnx -Uq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAv6ASu2-49G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 确保确定性行为\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
        "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
        "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
        "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
        "\n",
        "# 设备配置\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 从 MNIST 镜像列表中移除慢速镜像\n",
        "torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\n",
        "                                      if not mirror.startswith(\"http://yann.lecun.com\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziS-jmVl-49G"
      },
      "source": [
        "### 1️⃣ 步骤 1：导入 W&B 并登录"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xocZS1xv-49G"
      },
      "source": [
        "为了将数据记录到我们的 Web 服务，你需要登录。  \n",
        "如果你是第一次使用 W&B，你需要在出现的链接中注册一个免费账户。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVXsGwwl-49G"
      },
      "outputs": [],
      "source": [
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSLC9dst-49G"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HztxJ4g5-49G"
      },
      "source": [
        "# 👩‍🔬 定义实验和管道"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVlQzRDv-49G"
      },
      "source": [
        "## 2️⃣ 步骤 2：使用 `wandb.init` 跟踪元数据和超参数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9x8dxd_-49G"
      },
      "source": [
        "在编程中，我们首先定义实验：超参数是什么？与此运行相关的元数据是什么？  \n",
        "\n",
        "通常，我们会将这些信息存储在 `config` 字典（或类似对象）中，然后根据需要访问它。  \n",
        "\n",
        "在这个例子中，我们只让少数超参数变化，其余部分手动编码。但你的模型的任何部分都可以成为 `config` 的一部分！  \n",
        "\n",
        "我们还包括一些元数据：我们使用的是 MNIST 数据集和卷积架构。如果我们以后在同一项目中使用全连接架构处理 CIFAR，这将帮助我们区分运行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeO24eBu-49G"
      },
      "outputs": [],
      "source": [
        "config = dict(\n",
        "    epochs=5,\n",
        "    classes=10,\n",
        "    kernels=[16, 32],\n",
        "    batch_size=128,\n",
        "    learning_rate=0.005,\n",
        "    dataset=\"MNIST\",\n",
        "    architecture=\"CNN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ci-xoOC-49G"
      },
      "source": [
        "现在，让我们定义整个管道，这对于模型训练来说非常典型：  \n",
        "\n",
        "1. 首先，我们 `make` 一个模型，以及相关的数据和优化器，然后  \n",
        "2. 我们 `train` 模型，最后  \n",
        "3. `test` 它，看看训练效果如何。  \n",
        "   我们将在下面实现这些函数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OojyS1YO-49G"
      },
      "outputs": [],
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "\n",
        "    # 告诉 wandb 开始\n",
        "    with wandb.init(project=\"pytorch-demo\", config=hyperparameters):\n",
        "      # 通过 wandb.config 访问所有超参数，确保日志与执行一致！\n",
        "      config = wandb.config\n",
        "\n",
        "      # 创建模型、数据和优化问题\n",
        "      model, train_loader, test_loader, criterion, optimizer = make(config)\n",
        "      print(model)\n",
        "\n",
        "      # 使用它们训练模型\n",
        "      train(model, train_loader, criterion, optimizer, config)\n",
        "\n",
        "      # 测试其最终性能\n",
        "      test(model, test_loader)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLqdsz9N-49G"
      },
      "source": [
        "与标准管道的唯一区别是，所有操作都在 `wandb.init` 的上下文中进行。调用此函数会在你的代码和我们的服务器之间建立通信线路。  \n",
        "\n",
        "将 `config` 字典传递给 `wandb.init` 会立即将所有信息记录到我们这里，因此你始终知道为实验设置了哪些超参数值。  \n",
        "\n",
        "为了确保你选择并记录的值始终是模型中使用的值，我们建议使用 `wandb.config` 的副本。查看下面的 `make` 定义以了解一些示例。  \n",
        "\n",
        "> *注意*：我们确保在不同的进程中运行代码，因此我们这边的任何问题（例如，巨型海怪攻击我们的数据中心）都不会导致你的代码崩溃。一旦问题解决（例如，海怪返回深海），你可以使用 `wandb sync` 记录数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T96ApCE-49H"
      },
      "outputs": [],
      "source": [
        "def make(config):\n",
        "    # 创建数据\n",
        "    train, test = get_data(train=True), get_data(train=False)\n",
        "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
        "    test_loader = make_loader(test, batch_size=config.batch_size)\n",
        "\n",
        "    # 创建模型\n",
        "    model = ConvNet(config.kernels, config.classes).to(device)\n",
        "\n",
        "    # 创建损失函数和优化器\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    return model, train_loader, test_loader, criterion, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyoDI-JC-49H"
      },
      "source": [
        "# 📡 定义数据加载和模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2QeR6U-49H"
      },
      "source": [
        "现在，我们需要指定如何加载数据以及模型的外观。  \n",
        "\n",
        "这部分非常重要，但它与没有 `wandb` 时没有什么不同，因此我们不会过多讨论。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp8EEPy6-49H"
      },
      "outputs": [],
      "source": [
        "def get_data(slice=5, train=True):\n",
        "    full_dataset = torchvision.datasets.MNIST(root=\".\",\n",
        "                                              train=train,\n",
        "                                              transform=transforms.ToTensor(),\n",
        "                                              download=True)\n",
        "    # 等同于使用 [::slice] 切片\n",
        "    sub_dataset = torch.utils.data.Subset(\n",
        "      full_dataset, indices=range(0, len(full_dataset), slice))\n",
        "\n",
        "    return sub_dataset\n",
        "\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=True,\n",
        "                                         pin_memory=True, num_workers=2)\n",
        "    return loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5563i0mO-49H"
      },
      "source": [
        "定义模型通常是乐趣所在！  \n",
        "\n",
        "但使用 `wandb` 时没有任何变化，因此我们将坚持使用标准的 ConvNet 架构。  \n",
        "\n",
        "不要害怕尝试一些实验——你所有的结果都会记录在 [wandb.ai](https://wandb.ai) 上！  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyEEbv5N-49H"
      },
      "outputs": [],
      "source": [
        "# 传统卷积神经网络\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, kernels, classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, kernels[1], kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(7 * 7 * kernels[-1], classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyD1OyJr-49H"
      },
      "source": [
        "# 👟 定义训练逻辑"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2oO5GtZ-49H"
      },
      "source": [
        "继续我们的 `model_pipeline`，现在是时候指定如何 `train` 了。  \n",
        "\n",
        "这里有两个 `wandb` 函数发挥作用：`watch` 和 `log`。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8imMwkcw-49H"
      },
      "source": [
        "### 3️⃣ 步骤 3. 使用 `wandb.watch` 跟踪梯度，使用 `wandb.log` 跟踪其他内容"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn24L7rM-49H"
      },
      "source": [
        "`wandb.watch` 会记录模型的梯度和参数，每 `log_freq` 步训练一次。  \n",
        "\n",
        "你只需要在开始训练之前调用它。  \n",
        "\n",
        "其余的训练代码保持不变：我们遍历 epoch 和 batch，运行前向和后向传播，并应用我们的 `optimizer`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FkJnFWG-49H"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, criterion, optimizer, config):\n",
        "    # 告诉 wandb 监视模型的梯度、权重等！\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    # 运行训练并使用 wandb 跟踪\n",
        "    total_batches = len(loader) * config.epochs\n",
        "    example_ct = 0  # 已看到的样本数\n",
        "    batch_ct = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        for _, (images, labels) in enumerate(loader):\n",
        "\n",
        "            loss = train_batch(images, labels, model, optimizer, criterion)\n",
        "            example_ct +=  len(images)\n",
        "            batch_ct += 1\n",
        "\n",
        "            # 每 25 个 batch 报告一次指标\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "\n",
        "\n",
        "def train_batch(images, labels, model, optimizer, criterion):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # 前向传播 ➡\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # 后向传播 ⬅\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # 使用优化器更新\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pUSdZ-l-49H"
      },
      "source": [
        "唯一的区别在于日志代码：  \n",
        "以前你可能通过打印到终端来报告指标，现在你将相同的信息传递给 `wandb.log`。  \n",
        "\n",
        "`wandb.log` 期望一个以字符串为键的字典。这些字符串标识被记录的对象，它们构成了值。你还可以选择记录你处于训练的哪个 `step`。  \n",
        "\n",
        "> *注意*：我喜欢使用模型看到的样本数，因为这使得跨 batch 大小的比较更容易，但你可以使用原始步骤或 batch 计数。对于较长的训练运行，按 `epoch` 记录也是有意义的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXI3oxQV-49H"
      },
      "outputs": [],
      "source": [
        "def train_log(loss, example_ct, epoch):\n",
        "    # 魔法发生的地方\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
        "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEIjCS8A-49H"
      },
      "source": [
        "# 🧪 定义测试逻辑"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqL4hopx-49H"
      },
      "source": [
        "一旦模型完成训练，我们想要测试它：  \n",
        "可能是针对生产中的一些新数据运行它，或者将其应用于一些手工挑选的“困难样本”。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To3GtbLe-49H"
      },
      "source": [
        "#### 4️⃣ 可选步骤 4：调用 `wandb.save`  \n",
        "\n",
        "这也是将模型的架构和最终参数保存到磁盘的好时机。  \n",
        "为了最大兼容性，我们将以 [Open Neural Network eXchange (ONNX) 格式](https://onnx.ai/) `export` 我们的模型。  \n",
        "\n",
        "将该文件名传递给 `wandb.save` 可确保模型参数保存到 W&B 的服务器：不再丢失哪个 `.h5` 或 `.pb` 对应哪个训练运行！  \n",
        "\n",
        "有关存储、版本控制和分发模型的更高级 `wandb` 功能，请查看我们的 [Artifacts 工具](https://www.wandb.com/artifacts)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDscFgq2-49I"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    # 在测试样本上运行模型\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"Accuracy of the model on the {total} \" +\n",
        "              f\"test images: {correct / total:%}\")\n",
        "\n",
        "        wandb.log({\"test_accuracy\": correct / total})\n",
        "\n",
        "    # 以可交换的 ONNX 格式保存模型\n",
        "    torch.onnx.export(model, images, \"model.onnx\")\n",
        "    wandb.save(\"model.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C11VDi4-49L"
      },
      "source": [
        "# 🏃‍♀️ 运行训练并在 wandb.ai 上实时查看你的指标！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eS-nbWT-49L"
      },
      "source": [
        "现在我们已经定义了整个管道并插入了那几行 W&B 代码，我们准备运行完全跟踪的实验。  \n",
        "\n",
        "我们会向你报告一些链接：  \n",
        "我们的文档、  \n",
        "项目页面（组织项目中的所有运行）和  \n",
        "运行页面（存储此运行的结果）。  \n",
        "\n",
        "导航到运行页面并查看这些选项卡：  \n",
        "\n",
        "1. **图表**，其中记录了模型梯度、参数值和训练期间的损失  \n",
        "2. **系统**，其中包含各种系统指标，包括磁盘 I/O 利用率、CPU 和 GPU 指标（观察温度飙升 🔥）等  \n",
        "3. **日志**，其中包含训练期间推送到标准输出的任何内容的副本  \n",
        "4. **文件**，在训练完成后，你可以点击 `model.onnx` 使用 [Netron 模型查看器](https://github.com/lutzroeder/netron) 查看我们的网络。  \n",
        "\n",
        "一旦运行完成（即退出 `with wandb.init` 块），我们还会在单元格输出中打印结果的摘要。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifPhwl2k-49M"
      },
      "outputs": [],
      "source": [
        "# 使用管道构建、训练和分析模型\n",
        "model = model_pipeline(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayOgONmE-49M"
      },
      "source": [
        "# 🧹 使用 Sweeps 测试超参数\n",
        "\n",
        "在这个例子中，我们只查看了一组超参数。  \n",
        "但大多数 ML 工作流程的一个重要部分是迭代多个超参数。  \n",
        "\n",
        "你可以使用 Weights & Biases Sweeps 自动化超参数测试，并探索可能的模型和优化策略的空间。  \n",
        "\n",
        "## [查看使用 W&B Sweeps 在 PyTorch 中进行超参数优化 $→$](http://wandb.me/sweeps-colab)  \n",
        "\n",
        "使用 Weights & Biases 运行超参数扫描非常简单。只需 3 个简单步骤：  \n",
        "\n",
        "1. **定义扫描**：我们通过创建字典或 [YAML 文件](https://docs.wandb.com/library/sweeps/configuration) 来指定要搜索的参数、搜索策略、优化指标等。  \n",
        "\n",
        "2. **初始化扫描**：  `sweep_id = wandb.sweep(sweep_config)`  \n",
        "\n",
        "3. **运行扫描代理**：  `wandb.agent(sweep_id, function=train)`  \n",
        "\n",
        "就这样！这就是运行超参数扫描的全部内容！  \n",
        "<img src=\"https://imgur.com/UiQKg0L.png\" alt=\"Weights & Biases\" />  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNJo3BhL-49M"
      },
      "source": [
        "# 🖼️ 示例画廊\n",
        "\n",
        "查看使用 W&B 跟踪和可视化的项目示例，请访问我们的 [画廊 →](https://app.wandb.ai/gallery)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfsqVHuH-49M"
      },
      "source": [
        "# 🤓 高级设置  \n",
        "1. [环境变量](https://docs.wandb.com/library/environment-variables)：在环境变量中设置 API 密钥，以便你可以在托管集群上运行训练。  \n",
        "2. [离线模式](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline)：使用 `dryrun` 模式离线训练并稍后同步结果。  \n",
        "3. [本地部署](https://docs.wandb.com/self-hosted)：在私有云或隔离服务器中安装 W&B。我们为从学术界到企业团队的每个人提供本地安装。  \n",
        "4. [Sweeps](https://docs.wandb.com/sweeps)：使用我们的轻量级工具快速设置超参数搜索。  \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
