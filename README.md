# 文件说明

## 001【大模型LoRa微调】Qwen2.5 Coder 32B 指令微调



> 本文从零开始介绍了如何对“Qwen2.5 Coder 32B”模型进行LoRA指令微调，详细展示了环境准备、数据处理、微调步骤、推理与模型保存等流程。通过LoRA微调技术，我们可以在有限的硬件资源下针对特定指令场景（例如代码类问答、代码生成等）快速得到一个表现良好的大模型，而无需修改或更新海量的原始权重。

# 文件夹说明

## 002【W&B 深度学习实验管理】Jupyter Notebook

> 在机器学习项目的开发过程中，实验跟踪和结果可视化是至关重要的环节。无论是调整超参数、优化模型架构，还是监控训练过程中的性能变化，清晰的记录和直观的可视化都能显著提升开发效率。然而，许多开发者在实际操作中往往忽视了这一点，导致实验结果难以复现，或者在项目协作中出现混乱。今天，我们将介绍如何利用 Weights & Biases 这一强大的工具，与 PyTorch 深度集成，轻松实现实验跟踪、数据版本控制和团队协作。通过本文，你将学会如何在自己的项目中快速添加这一功能，让每一次实验都清晰可溯，每一次优化都有据可依。

## 003 【DeepSeek】R1 出圈算法复现 GRPO强化学习

> 【Deep Sig实践教程】手把手带你跑通模型训练+理论解析！🎯
>
> 本期视频从实战出发，带你一步步配置代码、优化显存、调整关键参数，完成基于GRPO算法的模型训练！💻 无论你是48G显存大佬还是7G显存入门党，都能找到适配方案。代码+Notebook已开源（评论区自取），实践部分涵盖梯度累积、奖励值收敛、KL散度防遗忘等硬核技巧，理论部分详解监督微调(SFT)、人类反馈强化学习(RLHF)与GRPO优化原理，揭秘大模型如何“对齐人类偏好”！
>
> 🔥 亮点速看：
>  1️⃣ 显存占用优化：梯度累积×batch size动态平衡
>  2️⃣ GRPO实战：组内对比生成+奖励模型联动，省显存利器
>  3️⃣ 测试彩蛋：鸡兔同笼轻松解，文学问题翻车现场（鲁智深乱入红楼梦？）
>  4️⃣ 完整训练闭环：从预训练本质到推理模板设计，拒绝“八股文”生成
